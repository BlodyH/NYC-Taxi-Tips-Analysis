{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/22 02:02:38 WARN Utils: Your hostname, Luo resolves to a loopback address: 127.0.1.1; using 172.17.1.121 instead (on interface eth0)\n",
      "22/08/22 02:02:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/22 02:02:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from urllib.request import urlretrieve\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.executor.memory\",\"4G\")\n",
    "    .config(\"spark.driver.memory\",\"8G\")\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['PULocationID', 'trip_distance', 'fare_amount', 'average_speed', 'time_duration', \\\n",
    "   'Temperature (F)', 'Wind Speed (mph)', 'Pickup_Time', 'Is_Airport', 'Is_Weekend', 'Is_Rainy']\n",
    "features = 'features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorise the features that are going to be put into the model\n",
    "def feature_converter(sdf):\n",
    "    vecAss = VectorAssembler(\n",
    "    # features to be used\n",
    "    inputCols=FEATURES, \n",
    "    # name of the output column\n",
    "    outputCol=features\n",
    "    )\n",
    "    df_va = vecAss.transform(sdf)\n",
    "    return df_va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# act as a pipline before modeling\n",
    "def preparation(sdf):\n",
    "    # vectorisation\n",
    "    sdf = feature_converter(sdf)\n",
    "    # take only the feature vector and the response\n",
    "    sdf = sdf.select([features, 'tip_amount'])\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read in the train test dataframe\n",
    "train_sdf = spark.read.parquet('../data/curated/tlc_data/Model_data.parquet')\n",
    "test_sdf = spark.read.parquet('../data/curated/tlc_data/Test_data.parquet')\n",
    "# pipline the dataframes\n",
    "train_sdf = preparation(train_sdf)\n",
    "test_sdf = preparation(test_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretisation the Tip amount into labels of High and Low\n",
    "from pyspark.sql.types import IntegerType\n",
    "@F.udf(IntegerType())\n",
    "def discretisation(tip):\n",
    "    if tip <= 2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the discretisation\n",
    "train_sdf = train_sdf.withColumn('label', discretisation(F.col('tip_amount')))\n",
    "test_sdf = test_sdf.withColumn('label', discretisation(F.col('tip_amount')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6310719090384228"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline accuracy\n",
    "train_sdf.where(F.col('label') == 1).count()/train_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "#Apply the logistic regression model\n",
    "\n",
    "log_reg=LogisticRegression(labelCol='label', featuresCol = features, elasticNetParam=1, family=\"binomial\").fit(train_sdf)\n",
    "result = log_reg.transform(test_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# define the values for error analysis\n",
    "TN = result.where((F.col('label') == 0) & (F.col('prediction') == 0)).count()\n",
    "FP = result.where((F.col('label') == 0) & (F.col('prediction') == 1)).count()\n",
    "FN = result.where((F.col('label') == 1) & (F.col('prediction') == 0)).count()\n",
    "TP = result.where((F.col('label') == 1) & (F.col('prediction') == 1)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy is: 0.7619080133355771\n",
      "The Recall is: 0.8640002122198863\n",
      "The Precision is: 0.7889589351355607\n",
      "The F1 score is: 0.824776206333329\n"
     ]
    }
   ],
   "source": [
    "# produce evaluation scores\n",
    "acc = (TN + TP)/(TN + TP + FN + FP)\n",
    "recall = TP/(TP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "F1 = 2*recall*precision / (recall + precision)\n",
    "print(f'The Accuracy is: {acc}')\n",
    "print(f'The Recall is: {recall}')\n",
    "print(f'The Precision is: {precision}')\n",
    "print(f'The F1 score is: {F1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
