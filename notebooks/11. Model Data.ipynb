{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/22 01:55:32 WARN Utils: Your hostname, Luo resolves to a loopback address: 127.0.1.1; using 172.17.1.121 instead (on interface eth0)\n",
      "22/08/22 01:55:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/22 01:55:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from urllib.request import urlretrieve\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)  \n",
    "    .config(\"spark.executor.memory\",\"4G\")\n",
    "    .config(\"spark.driver.memory\",\"8G\")\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files that are going to be used in modelling\n",
    "files = ['../data/curated/tlc_data/2021/final_data/2021--06-final_merged.parquet', \\\n",
    "        '../data/curated/tlc_data/2021/final_data/2021--07-final_merged.parquet', \\\n",
    "        '../data/curated/tlc_data/2021/final_data/2021--08-final_merged.parquet',\\\n",
    "        '../data/curated/tlc_data/2021/final_data/2021--09-final_merged.parquet',\\\n",
    "        '../data/curated/tlc_data/2021/final_data/2021--10-final_merged.parquet',\\\n",
    "        '../data/curated/tlc_data/2021/final_data/2021--11-final_merged.parquet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file that is going to be predicted\n",
    "predict = '../data/curated/tlc_data/2021/final_data/2021--12-final_merged.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final change of datatype to meet the requirements of the modelling\n",
    "def change_type(sdf):\n",
    "        return sdf.withColumn('is_airport', col('is_airport').cast(\"integer\"))\\\n",
    "                .withColumn('is_rainy', (col('is_rainy') == 'True').cast('integer'))\\\n",
    "                .withColumn('Temperature (F)', col('Temperature (F)').cast('float'))\\\n",
    "                .withColumn('Wind Speed (mph)', col('Wind Speed (mph)').cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the categorical features so that they could fit into a numeric model\n",
    "def one_hot(sdf):\n",
    "    ohe = OneHotEncoder()\n",
    "    # attributes that need to be encoded\n",
    "    ohe.setInputCols([\"is_weekend\", 'is_airport', 'is_rainy', 'Pickup_time', 'Month'])\n",
    "    ohe.setOutputCols([\"Is_Weekend\", \"Is_Airport\", \"Is_Rainy\", 'Pickup_Time', 'MONTH'])\n",
    "    model = ohe.fit(sdf)\n",
    "    # encode the dataset\n",
    "    sdf = model.transform(sdf)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path = \"../data/curated/tlc_data/\"\n",
    "# read in all files for training\n",
    "sdf = spark.read.parquet(*files)\n",
    "sdf = change_type(sdf)\n",
    "sdf = one_hot(sdf)\n",
    "# save as the final model data\n",
    "sdf.write.mode(\"overwrite\").parquet(f'{path}/Model_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read in the file for testing\n",
    "sdf = spark.read.parquet(predict)\n",
    "sdf = change_type(sdf)\n",
    "sdf = one_hot(sdf)\n",
    "# save sa the final test data\n",
    "sdf.write.mode(\"overwrite\").parquet(f'{path}/Test_data.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
